
\name{krmm}
\alias{krmm}
%- Also NEED an '\alias' for EACH other topic documented here.
\encoding{utf8}

\title{
  Kernel ridge regression in the mixed model framework
}

\description{
krmm solves kernel ridge regression for various kernels within the following mixed model framework: $Y =X\beta + ZU + \varepsilon$, where	X and Z correspond to the design matrices of predictors with fixed and random effects respectively.
}

\usage{
	krmm(Y, X = rep(1, length(Y)), Z = diag(1, length(Y)),
           Matrix_covariates, method = "RKHS", kernel = "Gaussian",
           rate_decay_kernel = 0.1, degree_poly = 2, scale_poly = 1,
           offset_poly = 1, degree_anova = 3,
           init_sigma2K = 2, init_sigma2E = 3, convergence_precision = 1e-08,
           nb_iter = 1000, display = F)
	}

%- maybe also 'usage' for other objects documented here.
\arguments{

	\item{Y}{numeric vector; response vector for training data}

	\item{X}{numeric matrix; design matrix of predictors with fixed effects for training data (default is a vector of ones)}

	\item{Z}{numeric matrix; design matrix of predictors with random effects for training data (default is identity matrix)}

	\item{Matrix_covariates}{numeric matrix of entries used to build the kernel matrix}

	\item{method}{character string; RKHS, GBLUP or RR-BLUP}

	\item{kernel}{character string; Gaussian, Laplacian or ANOVA (kernels for RKHS regression ONLY, the linear kernel is automatically built for GBLUP and RR-BLUP and hence no kernel is supplied for these methods) }

	\item{rate_decay_kernel}{numeric scalar; hyperparameter of the Gaussian, Laplacian or ANOVA kernel (default is 0.1)}

	\item{degree_poly, scale_poly, offset_poly}{numeric scalars; parameters for polynomial kernel (defaults are 2, 1 and 1 respectively)}

	\item{degree_anova}{numeric scalar; parameter for ANOVA kernel (defaults is 3)}

	\item{init_sigma2K, init_sigma2E}{numeric scalars;
		initial guess values, associated to the mixed model variance parameters, for the EM-REML algorithm
		(defaults are 2 and 3 respectively)
		}

	\item{convergence_precision, nb_iter}{
		numeric scalars; convergence precision (i.e. tolerance) associated to the mixed model variance parameters,
		for the EM-REML algorithm, and number of maximum iterations allowed if
		convergence is not reached (defaults are 1e-8 and 1000 respectively)
	}

	\item{display}{
		boolean (TRUE or FALSE character string);
		should estimated components be displayed at each
		iteration
	}

}

\details{
The matrix Matrix_covariates is mandatory to build the kernel matrix for model estimation, and prediction (see predict_krmm).
}

\value{

  \item{beta_hat}{estimated fixed effect(s)}

  \item{sigma2K_hat, sigma2E_hat}{estimated variance components}

  \item{vect_alpha}{estimated dual variables}

  \item{gamma_hat}{RR-BLUP of covariates effects (i.e. available for RR-BLUP method only)}
}

\references{

Jacquin et al. (2016). A unified and comprehensible view of parametric and kernel methods for genomic prediction with application to rice (in peer review).

Robinson, G. K. (1991). That blup is a good thing: the estimation of random effects. Statistical science, 534 15-32

Foulley, J.-L. (2002). Algorithme em: théorie et application au modèle mixte. Journal de la Société française de Statistique 143, 57-109

}

\author{Laval Jacquin <jacquin.julien@gmail.com>}

\examples{

# load libraries
library(KRMM)
library(MASS)
library(kernlab)
library(cvTools)

# simulate data
set.seed(123)
p <- 100
n <- 200
beta <- rnorm(p, mean = 0, sd = 1.0) # random effects
X <- matrix(runif(p * n, min = 0, max = 1), ncol = p, byrow = T) # matrix of covariates
f <- tcrossprod(beta, X) # data generating process
eps <- rnorm(n, mean = 0, sd = 0.9) # add residuals
Y <- f + eps

# split data into training and test set
n_train <- floor(n * 0.67)
idx_train <- sample(1:n, size = n_train, replace = F)

# train
x_train <- X[idx_train, ]
y_train <- Y[idx_train]
length(y_train)

# test
x_test <- X[-idx_train, ]
y_test <- Y[-idx_train]
f_test <- f[-idx_train] # true value generated by DGP we want to predict
length(y_test)

# train and predict with krmm linear kernel
linear_krmm_model <- krmm(
  Y = y_train, Matrix_covariates = x_train,
  method = "RR-BLUP"
)

# without fixed effects
f_hat_test <- predict_krmm(linear_krmm_model,
  Matrix_covariates = x_test
)

dev.new()
plot(f_hat_test, f_test,
  main = "Linear RKHS regression with default rate of decay (not optimized)"
)
cor(f_hat_test, f_test)

# add fixed effects
f_hat_test <- predict_krmm(linear_krmm_model,
  Matrix_covariates = x_test, add_flxed_effects = T
)

dev.new()
plot(f_hat_test, f_test,
  main = "Linear RKHS regression with default rate of decay (not optimized)"
)
cor(f_hat_test, f_test)

# train and predict with krmm gaussian kernel (default kernel for RKHS method)
non_linear_krmm_model <- krmm(
  Y = y_train, Matrix_covariates = x_train,
  method = "RKHS"
)

# without fixed effects
f_hat_test <- predict_krmm(non_linear_krmm_model,
  Matrix_covariates = x_test
)

dev.new()
plot(f_hat_test, f_test,
  main = "Gaussian RKHS regression with default rate of decay (not optimized)"
)
cor(f_hat_test, f_test)

# add fixed effects
f_hat_test <- predict_krmm(non_linear_krmm_model,
  Matrix_covariates = x_test, add_flxed_effects = T
)

dev.new()
plot(f_hat_test, f_test,
  main = "Gaussian RKHS regression with default rate of decay (not optimized)"
)
cor(f_hat_test, f_test)

\dontrun{

# tune krmm model with a gaussian kernel
non_linear_opt_krmm_obj <- tune_krmm(
  Y = y_train, Matrix_covariates = x_train,
  rate_decay_grid = seq(0.01, 0.1, length.out = 5), nb_folds = 3,
  method = "RKHS"
)
non_linear_opt_krmm_obj$optimal_h

plot(non_linear_opt_krmm_obj$rate_decay_grid,
  non_linear_opt_krmm_obj$mean_loss_grid,
  type = "l"
)

non_linear_opt_krmm_model <- non_linear_opt_krmm_obj$optimized_model

# add fixed effects
f_hat_test <- predict_krmm(non_linear_opt_krmm_model,
  Matrix_covariates = x_test, add_flxed_effects = T
)
dev.new()
plot(f_hat_test, f_test,
  main = "Gaussian RKHS regression with optimized rate of decay"
)
cor(f_hat_test, f_test)

}

}
